\section{Methods}

In my study, I used a machine learning method called topic analysis on the entire data set to identify the subject of each project. One of the main advantages of this technique is the efficiency and accuracy of topic analysis when examining large and complex datasets. At the same time, topic analysis usually provides a more objective result, as it can identify underlying themes from texts independently of potentially biased perspectives, which may exist in keywords provided by applicants [\cite{hagen2018content}]. Subsequently, the results after the application of topic modelling were classified using ChatGPT due to its consistency, efficiency, and reduction of human error compared to manual classification. This classified data will be used for further analysis. After identifying the gender of the lead author for each project, the final step involves quantifying the overall gender proportions and comparing these across different STEM categories over time.

To account for all biases, including before the application stage, the ratio is calculated by dividing the funded males or females by the total number of staff in each classification for comparison.

\subsection{Data acquisition and characteristics}
I used three datasets. The first consisted of title abstracts from 107,760 projects collected from UKRI, provided by our colleague [\cite{Flavia}]. The second dataset contained relevant project information, which I obtained from UKRI, including the leading institution, the applicant's name, the start and end date of the project, etc. These two raw datasets were merged for further analysis.

The third data set is on the information of the entire academic staff in the UK STEM area, sourced from the website of the Higher Education Statistics Agency (HESA). This includes the gender data of staff in various academic fields. As the HESA data were available only from 2015 to 2022 at the time of our study was taken, we limited our analysis to title abstract data within this period.

\subsection{Procedures}

\subsubsection{Data processing}

The first step of our study is the pre-processing of the metadata. The raw dataset of the title abstracts is cleaned by several approaches, including filtering of STEM funding bodies and the selection of valuable columns. The information in the clean UK data includes the project ID, funding body, the lead institution, the applicant's name, the date of the project, and the funding amount. In addition, the title abstracts are tokenized and filtered by dropping the rows where the number of tokens is less than 9, selecting only the data that provide meaningful information.
\bigbreak
\noindent After completing the pre-processing of the raw metadata, I used Natural Language Processing (NLP), particularly topic analysis, on the cleaned data for the identification of topics.  Topic analysis is a machine learning technique to identify topics by finding common themes in vast amounts of text [\cite{Flavia}]. Specifically, the Latent Dirichlet Allocation (LDA) model was applied to our cleaned data to find the underlying topic of each project in our dataset. As an example of topic analysis, LDA can detect the underlying topics in a collection of documents and then determine how likely they belong to each topic by generating a likelihood distribution result [\cite{wikipedia2023latent}]. We applied LDA models with 50 to 200 topics in 25 increments to the data, and calculated the perplexity value for each of the models with different numbers of topics to determine the best-fitted model. The model with 200 topics is finally selected due to its lowest perplexity value compared to the others, indicating better predictive performance. 

\subsubsection{Analysis}
\noindent Upon applying LDA to the clean data, I obtained the conditional probability distribution of each project belonging to each of the 200 topics. I choose the topic with the highest probability as the corresponding topic for each project. The next step is the classification of the issues. The results generated from LDA involve a comprehensive collection of keywords for each topic. To enhance efficiency and accuracy, I use the advanced machine learning tool, ChatGPT, to sort the 200 topics into the following STEM categories according to their keywords: Veterinary Science; Agriculture, Forestry \& Food Science; General Engineering; Chemical Engineering; Mineral, Metallurgy \& Materials Engineering; Civil Engineering; Electrical, Electronic \& Computer Engineering; Mechanical, Aero \& Production Engineering; IT, Systems Sciences \& Computer Software Engineering; Earth, Marine \& Environmental Sciences; Biosciences; Physics; Chemistry; Mathematics.\\
\\
To determine the gender for each project, I applied a Python package, Gender Guessor, to the leading applicant of each project in the clean data to distinguish their gender based on their names. The output of this Python package includes Male, Female, Mostly male, Mostly female, Unknown, and Androgynous.  In this study, both "Male" and "Mostly male" outputs are classified as male, while "Female" and "Mostly female" are treated as female, and the other results are not considered due to the higher uncertainty and inaccuracy. The gender information is combined with the classification of each project. The combined dataset is then used for further analysis, aiming to address the following segments:


\textbf{1. Comparison of the average funding amount in 2015-2022}

To detect the existence of a gender gap, the first step is to examine the distribution of funding amounts between genders. Although the government has announced significant funding for STEM, the proportion allocated to females remains unknown. It is crucial to determine whether each gender has equal access to funding. Therefore, the initial task of this study is to assess the average amount of funding for each gender. This involves identifying the total amount of funding during the period and dividing it by the number of males and females funded, respectively. I will then compare the average funding distributed to each gender in the UK from 2015 to 2022 in various categories.

\textbf{2. Tracking gender-based funding trends in each STEM category}

Despite recent policy initiatives aimed at promoting STEM research through increased funding, it remains unclear whether these measures will effectively increase innovation among female researchers. Therefore, after identifying the general pattern, another objective of this study is to track the funding trends for each gender within each classification during the period and to determine whether there is a gender disparity and whether current policies have positively impacted it. The average funding identified in the first step will be compared over time.

\textbf{3. Comparison of funding bias ratio across different categories.}

The next phase involves comparing the degree of bias to identify which area may contribute the most to the potential disparity. A funding ratio for male and female researchers is computed by dividing the number of funded researchers within each gender by the total number of staff members of the same gender. In the above steps, we can detect the potential bias of the funded researchers. However, to understand the underlying reasons for the bias, it is essential to consider the bias at all stages, including before and after the application process - the potential reason for each gender satisfying the application requirement, the chance of each gender applied for funding that finally getting funded, as well as the general pattern for funded researhcers that would be addressed in the above questions, should all be considered. Therefore, this explains why my study uses the total number of academic staff to calculate the gender ratio. These ratios for each gender are compared over the years. Finally, the degree of bias across all categories is compared by calculating the difference in the gender ratios for each gender, referred to as the bias ratio. A higher value indicates a greater degree of gender bias.



